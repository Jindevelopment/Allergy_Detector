#ì´ ì½”ë“œëŠ” ocr testë¥¼ ìœ„í•œ ì½”ë“œì…ë‹ˆë‹¤. 
#!/usr/bin/env python3
# ocr_test.py
#source venv/bin/activate
# ocr_test_optimized.py
import cv2
import pytesseract
import numpy as np
from PIL import Image
import sys
import re
import easyocr
import time
import torch

def check_gpu_availability():
    """
    ë§¥ë¶ GPU ê°€ìš©ì„± í™•ì¸ í•¨ìˆ˜
    """
    print("ğŸ” ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸ ì¤‘...")
    
    # MPS (Metal Performance Shaders) í™•ì¸
    if torch.backends.mps.is_available():
        print("âœ… MPS(Metal Performance Shaders) ì‚¬ìš© ê°€ëŠ¥!")
        print("ğŸš€ ë§¥ë¶ M1/M2/M3 ì¹© GPU ê°€ì† ì§€ì›")
        return True
    elif torch.cuda.is_available():
        print("âœ… CUDA GPU ì‚¬ìš© ê°€ëŠ¥!")
        return True
    else:
        print("âŒ GPU ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
        return False

def apply_advanced_preprocessing(image):
    """
    ê³ ê¸‰ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ í•¨ìˆ˜ (Subeen lab ë¸”ë¡œê·¸ ì°¸ê³ )
    https://subeen-lab.tistory.com/121
    """
    # 1ï¸âƒ£ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    
    # 2ï¸âƒ£ ë‹¤ì–‘í•œ Blur ë°©ë²•ë“¤ ì ìš©
    # Gaussian Blur
    gaussian_blur = cv2.GaussianBlur(gray, (5, 5), 0)
    
    # Median Blur (ë…¸ì´ì¦ˆ ì œê±°ì— íš¨ê³¼ì )
    median_blur = cv2.medianBlur(gray, 5)
    
    # Bilateral Filter (ì—£ì§€ ë³´ì¡´í•˜ë©´ì„œ ë…¸ì´ì¦ˆ ì œê±°)
    bilateral_blur = cv2.bilateralFilter(gray, 9, 75, 75)
    
    # 3ï¸âƒ£ íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™”
    equalized = cv2.equalizeHist(gray)
    
    # 4ï¸âƒ£ CLAHE (ì ì‘í˜• íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™”)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    clahe_applied = clahe.apply(gray)
    
    # 5ï¸âƒ£ ì±„ë„ ì¡°ì ˆ (Saturation Adjustment)
    # HSV ìƒ‰ìƒ ê³µê°„ìœ¼ë¡œ ë³€í™˜
    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    # ì±„ë„ ì¦ê°€ (1.5ë°°)
    hsv_image[:, :, 1] = np.clip(hsv_image[:, :, 1] * 1.5, 0, 255)
    # ë‹¤ì‹œ BGRë¡œ ë³€í™˜
    saturated_image = cv2.cvtColor(hsv_image, cv2.COLOR_HSV2BGR)
    saturated_gray = cv2.cvtColor(saturated_image, cv2.COLOR_BGR2GRAY)
    
    return {
        'original': gray,
        'gaussian_blur': gaussian_blur,
        'median_blur': median_blur,
        'bilateral_blur': bilateral_blur,
        'equalized': equalized,
        'clahe': clahe_applied,
        'saturated': saturated_gray
    }

def apply_morphology_operations(image):
    """
    ëª¨í´ë¡œì§€ ì—°ì‚° ì ìš© (ì¹¨ì‹, íŒ½ì°½, ë‹«í˜, ì—´ë¦¼)
    """
    # ì»¤ë„ ìƒì„±
    kernel = np.ones((3, 3), np.uint8)
    
    # ì¹¨ì‹ ì—°ì‚° (í…ìŠ¤íŠ¸ë¥¼ ì–‡ê²Œ)
    erosion = cv2.erode(image, kernel, iterations=1)
    
    # íŒ½ì°½ ì—°ì‚° (í…ìŠ¤íŠ¸ë¥¼ ë‘ê»ê²Œ)
    dilation = cv2.dilate(image, kernel, iterations=1)
    
    # ë‹«í˜ ì—°ì‚° (í…ìŠ¤íŠ¸ ë‚´ë¶€ êµ¬ë© ë©”ìš°ê¸°)
    closing = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)
    
    # ì—´ë¦¼ ì—°ì‚° (ì‘ì€ ë…¸ì´ì¦ˆ ì œê±°)
    opening = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)
    
    return {
        'erosion': erosion,
        'dilation': dilation,
        'closing': closing,
        'opening': opening
    }

def apply_multiple_thresholding(image):
    """
    ë‹¤ì–‘í•œ ì´ì§„í™” ë°©ë²• ì ìš©
    """
    # OTSU ì´ì§„í™”
    _, thresh_otsu = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    
    # ì ì‘í˜• ì´ì§„í™” - Gaussian
    thresh_adapt_gaussian = cv2.adaptiveThreshold(
        image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2
    )
    
    # ì ì‘í˜• ì´ì§„í™” - Mean
    thresh_adapt_mean = cv2.adaptiveThreshold(
        image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2
    )
    
    # ì„ê³„ê°’ ì´ì§„í™”
    _, thresh_binary = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY)
    
    return {
        'otsu': thresh_otsu,
        'adapt_gaussian': thresh_adapt_gaussian,
        'adapt_mean': thresh_adapt_mean,
        'binary': thresh_binary
    }

def ocr_image_with_opencv(image, lang="kor+eng", fast_mode=True):
    """
    OpenCV + Tesseract OCR ìµœì í™” í•¨ìˆ˜ (ë‹¤ì–‘í•œ ì „ì²˜ë¦¬ ë°©ë²• ì ìš©)
    image: PIL.Image ë˜ëŠ” íŒŒì¼ ê²½ë¡œ
    lang: OCR ì–¸ì–´ ì„¤ì •
    fast_mode: ë¹ ë¥¸ ëª¨ë“œ (ê¸°ë³¸ê°’: True)
    """
    # ì´ë¯¸ì§€ê°€ PIL ê°ì²´ë¼ë©´ OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    if isinstance(image, Image.Image):
        image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
    elif isinstance(image, str):  # íŒŒì¼ ê²½ë¡œì¼ ê²½ìš°
        image = cv2.imread(image)
        if image is None:
            raise FileNotFoundError(f"ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image}")

    # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ì ë‹¹í•œ í¬ê¸°ë¡œ)
    height, width = image.shape[:2]
    if width > 1200 or height > 1200:
        scale = min(1200/width, 1200/height)
        new_width = int(width * scale)
        new_height = int(height * scale)
        image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)

    if fast_mode:
        # ğŸš€ ë¹ ë¥¸ ëª¨ë“œ: ê¸°ë³¸ ì „ì²˜ë¦¬ë§Œ
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        gray = cv2.equalizeHist(gray)
        blurred = cv2.GaussianBlur(gray, (3, 3), 0)
        _, thresh = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        config = "--psm 3 --oem 3 -c preserve_interword_spaces=1"
        
    else:
        # ğŸ¯ ì •ë°€ ëª¨ë“œ: ë‹¤ì–‘í•œ ì „ì²˜ë¦¬ ë°©ë²• ì‹œë„
        # ê³ ê¸‰ ì „ì²˜ë¦¬ ì ìš©
        processed_images = apply_advanced_preprocessing(image)
        
        # ê°€ì¥ íš¨ê³¼ì ì¸ ì´ë¯¸ì§€ ì„ íƒ (CLAHE ì ìš©ëœ ì´ë¯¸ì§€)
        best_image = processed_images['clahe']
        
        # ëª¨í´ë¡œì§€ ì—°ì‚° ì ìš©
        morph_images = apply_morphology_operations(best_image)
        
        # ê°€ì¥ íš¨ê³¼ì ì¸ ëª¨í´ë¡œì§€ ê²°ê³¼ ì„ íƒ (ë‹«í˜ ì—°ì‚°)
        best_morph = morph_images['closing']
        
        # ë‹¤ì–‘í•œ ì´ì§„í™” ë°©ë²• ì ìš©
        thresh_images = apply_multiple_thresholding(best_morph)
        
        # ê°€ì¥ íš¨ê³¼ì ì¸ ì´ì§„í™” ê²°ê³¼ ì„ íƒ (ì ì‘í˜• Gaussian)
        thresh = thresh_images['adapt_gaussian']
        
        # ì¶”ê°€ ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì˜ì—­ ê°•í™”
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))
        thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)
        
        config = "--psm 6 --oem 3 -c preserve_interword_spaces=1 -c tessedit_char_whitelist=0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyzê°€-í£():.,()[]{}"

    # OCR ìˆ˜í–‰ (í•œêµ­ì–´ ìš°ì„ )
    text = pytesseract.image_to_string(thresh, lang=lang, config=config)
    return text.strip()

def ocr_with_easyocr(image_path_or_object, lang=['ko', 'en']):
    """
    EasyOCRì„ ì‚¬ìš©í•œ ê³ ì„±ëŠ¥ OCR í•¨ìˆ˜ (í•œêµ­ì–´ íŠ¹í™” - ë§¥ë¶ GPU ì§€ì›)
    """
    try:
        # ë§¥ë¶ M1/M2/M3 ì¹©ì—ì„œ MPS(Metal Performance Shaders) ì‚¬ìš©
        import torch
        if torch.backends.mps.is_available():
            print("ğŸš€ ë§¥ë¶ GPU(MPS) ê°€ì† ì‚¬ìš© ì¤‘...")
            reader = easyocr.Reader(lang, gpu=True)  # MPS ì‚¬ìš©
        else:
            print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘...")
            reader = easyocr.Reader(lang, gpu=False)
        
        # ì´ë¯¸ì§€ ì½ê¸°
        if isinstance(image_path_or_object, str):
            image = cv2.imread(image_path_or_object)
        else:
            # PIL Imageë¥¼ numpy arrayë¡œ ë³€í™˜
            image = np.array(image_path_or_object)
            if len(image.shape) == 3 and image.shape[2] == 3:
                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        
        # EasyOCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        results = reader.readtext(image)
        
        # ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        extracted_text = ""
        for (bbox, text, confidence) in results:
            if confidence > 0.5:  # ì‹ ë¢°ë„ 50% ì´ìƒë§Œ ì‚¬ìš©
                extracted_text += text + " "
        
        return extracted_text.strip()
    
    except Exception as e:
        print(f"EasyOCR ì˜¤ë¥˜: {e}")
        return ""

def ocr_with_enhanced_preprocessing(image_path_or_object, use_easyocr=True):
    """
    í–¥ìƒëœ ì „ì²˜ë¦¬ + ê³ ì„±ëŠ¥ OCR í•¨ìˆ˜
    """
    try:
        # ì´ë¯¸ì§€ ì½ê¸°
        if isinstance(image_path_or_object, str):
            image = cv2.imread(image_path_or_object)
        else:
            image = np.array(image_path_or_object)
            if len(image.shape) == 3 and image.shape[2] == 3:
                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        
        if image is None:
            return ""
        
        # ê³ ê¸‰ ì „ì²˜ë¦¬ ì ìš©
        processed_images = apply_advanced_preprocessing(image)
        
        # ì—¬ëŸ¬ ì „ì²˜ë¦¬ ê²°ê³¼ ì¤‘ ê°€ì¥ ì¢‹ì€ ê²ƒë“¤ ì‹œë„
        best_images = [
            processed_images['clahe'],  # CLAHE ì ìš©
            processed_images['equalized'],  # íˆìŠ¤í† ê·¸ë¨ ê· ë“±í™”
            processed_images['bilateral_blur'],  # Bilateral Filter
            processed_images['original']  # ì›ë³¸
        ]
        
        all_texts = []
        
        if use_easyocr:
            # EasyOCR ì‚¬ìš© (ë§¥ë¶ GPU ì§€ì›)
            import torch
            if torch.backends.mps.is_available():
                print("ğŸš€ ë§¥ë¶ GPU(MPS) ê°€ì† ì‚¬ìš© ì¤‘...")
                reader = easyocr.Reader(['ko', 'en'], gpu=True)
            else:
                print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘...")
                reader = easyocr.Reader(['ko', 'en'], gpu=False)
            
            for img in best_images:
                try:
                    results = reader.readtext(img)
                    text = ""
                    for (bbox, detected_text, confidence) in results:
                        if confidence > 0.3:  # ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ë” ë§ì€ í…ìŠ¤íŠ¸ ìº¡ì²˜
                            text += detected_text + " "
                    if text.strip():
                        all_texts.append(text.strip())
                except:
                    continue
        else:
            # Tesseract ì‚¬ìš©
            for img in best_images:
                try:
                    # ëª¨í´ë¡œì§€ ì—°ì‚° ì ìš©
                    morph_images = apply_morphology_operations(img)
                    best_morph = morph_images['closing']
                    
                    # ì´ì§„í™”
                    thresh_images = apply_multiple_thresholding(best_morph)
                    thresh = thresh_images['adapt_gaussian']
                    
                    # OCR ìˆ˜í–‰
                    text = pytesseract.image_to_string(thresh, lang='kor+eng', config='--psm 6 --oem 3')
                    if text.strip():
                        all_texts.append(text.strip())
                except:
                    continue
        
        # ëª¨ë“  ê²°ê³¼ë¥¼ í•©ì¹˜ê³   ì œê±°
        combined_text = " ".join(all_texts)
        return combined_text.strip()
    
    except Exception as e:
        print(f"í–¥ìƒëœ OCR ì˜¤ë¥˜: {e}")
        return ""

def extract_ingredients_from_text(text):
    """
    OCRë¡œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì—ì„œ ì›ì¬ë£Œëª…ë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ (í•œêµ­ ì‹í’ˆ ì„±ë¶„í‘œ íŠ¹í™” - ê°œì„ ëœ ë²„ì „)
    """
    ingredients = []
    
    # ë¨¼ì € ì§ì ‘ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì£¼ìš” ì¬ë£Œë“¤ ì¶”ì¶œ
    ingredients = extract_direct_patterns(text)
    
    # ì›ì¬ë£Œëª… ì„¹ì…˜ ì°¾ê¸°
    ingredient_section = find_ingredient_section(text)
    
    if ingredient_section:
        # ì›ì¬ë£Œëª… ì„¹ì…˜ì—ì„œ ì¶”ê°€ ì¶”ì¶œ
        section_ingredients = extract_from_ingredient_section(ingredient_section)
        ingredients.extend(section_ingredients)
    else:
        # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì›ì¬ë£Œëª… íŒ¨í„´ ì°¾ê¸°
        full_text_ingredients = extract_from_full_text(text)
        ingredients.extend(full_text_ingredients)
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    ingredients = list(set(ingredients))
    ingredients.sort()
    
    # ì•Œë ˆë¥´ê¸° ìœ ë°œ ì„±ë¶„ë§Œ í•„í„°ë§
    allergen_ingredients = filter_allergen_ingredients(ingredients)
    
    return allergen_ingredients

def extract_from_ingredient_section(section_text):
    """
    ì›ì¬ë£Œëª… ì„¹ì…˜ì—ì„œ ì„±ë¶„ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    ingredients = []
    
    # 1. ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì¬ë£Œëª…ë“¤ ì¶”ì¶œ (ê°€ì¥ ì¼ë°˜ì ì¸ íŒ¨í„´)
    comma_pattern = r'([ê°€-í£]+(?:\([^)]*\))?(?:\{[^}]*\})?)'
    matches = re.findall(comma_pattern, section_text)
    
    for match in matches:
        # ê° ì¬ë£Œëª…ì„ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬
        individual_ingredients = split_compound_ingredient(match)
        for ingredient in individual_ingredients:
            cleaned = clean_ingredient_text(ingredient)
            if cleaned and is_valid_ingredient(cleaned):
                ingredients.append(cleaned)
    
    # 2. íŠ¹ìˆ˜ íŒ¨í„´ë“¤ ì¶”ì¶œ
    special_patterns = [
        # ë°±ë¶„ìœ¨ì´ í¬í•¨ëœ ì¬ë£Œëª… (ì˜ˆ: ê°€ê³µìœ í¬ë¦¼31.86%)
        r'([ê°€-í£]+(?:\d+\.?\d*%)?)',
        # ê´„í˜¸ ì•ˆì— ìƒì„¸ ì •ë³´ê°€ ìˆëŠ” ì¬ë£Œëª…
        r'([ê°€-í£]+\([^)]*\))',
        # ì¤‘ê´„í˜¸ ì•ˆì— ë³µí•© ì •ë³´ê°€ ìˆëŠ” ì¬ë£Œëª…
        r'([ê°€-í£]+\{[^}]*\})',
    ]
    
    for pattern in special_patterns:
        matches = re.findall(pattern, section_text)
        for match in matches:
            cleaned = clean_ingredient_text(match)
            if cleaned and is_valid_ingredient(cleaned):
                ingredients.append(cleaned)
    
    return ingredients

def remove_duplicate_allergens(ingredients):
    """
    ì¤‘ë³µëœ ì•Œë ˆë¥´ê¸° ìœ ë°œ ì„±ë¶„ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)
    """
    if not ingredients:
        return []
    
    # ì•Œë ˆë¥´ê¸° ì„±ë¶„ ë§¤í•‘ (ì¤‘ë³µ ì œê±° ë° í‘œì¤€í™”)
    allergen_mapping = {
        # ë°€ ê´€ë ¨ - ë°€ê°€ë£¨ë¡œ í†µì¼
        'ë°€': 'ë°€ê°€ë£¨',
        'ë°€ê°€ë£¨': 'ë°€ê°€ë£¨',
        'ë°€ê¸€ë£¨í…': 'ë°€ê°€ë£¨',
        'ë°€ì „ë¶„': 'ë°€ê°€ë£¨',
        'ë°€íš¨ì†Œ': 'ë°€ê°€ë£¨',
        'ë°€ë‹¨ë°±ì§ˆ': 'ë°€ê°€ë£¨',
        
        # ë‚œë¥˜ ê´€ë ¨ - ë‹¬ê±€ë¡œ í†µì¼
        'ê³„ë€': 'ë‹¬ê±€',
        'ë‹¬ê±€': 'ë‹¬ê±€',
        'ì „ë€ì•¡': 'ë‹¬ê±€',
        'ë‚œí™©ì•¡': 'ë‹¬ê±€',
        'ë‚œë°±ë¶„': 'ë‹¬ê±€',
        'ë‚œí™©ë¶„ë§': 'ë‹¬ê±€',
        'ë‚œë°±ë¶„ë§': 'ë‹¬ê±€',
        
        # ìš°ìœ  ê´€ë ¨ - ìš°ìœ ë¡œ í†µì¼
        'ìš°ìœ ': 'ìš°ìœ ',
        'ì „ì§€ë¶„ìœ ': 'ìš°ìœ ',
        'íƒˆì§€ë¶„ìœ ': 'ìš°ìœ ',
        'ìœ í¬ë¦¼': 'ìš°ìœ ',
        'ê°€ê³µìœ í¬ë¦¼': 'ìš°ìœ ',
        'ìœ ë‹¹': 'ìš°ìœ ',
        'ìœ ì²­ë‹¨ë°±ë¶„ë§': 'ìš°ìœ ',
        'í˜¼í•©ë¶„ìœ ': 'ìš°ìœ ',
        'ì—°ìœ ': 'ìš°ìœ ',
        'ë²„í„°': 'ìš°ìœ ',
        'ë§ˆê°€ë¦°': 'ìš°ìœ ',
        'ì¹˜ì¦ˆ': 'ìš°ìœ ',
        'ìš”ê±°íŠ¸': 'ìš°ìœ ',
        
        # ëŒ€ë‘ ê´€ë ¨ - ëŒ€ë‘ë¡œ í†µì¼
        'ëŒ€ë‘': 'ëŒ€ë‘',
        'ì½©': 'ëŒ€ë‘',
        'ë‘ë¶€': 'ëŒ€ë‘',
        'ëœì¥': 'ëŒ€ë‘',
        'ê°„ì¥': 'ëŒ€ë‘',
        'ê³ ì¶”ì¥': 'ëŒ€ë‘',
        'ì½©ê¸°ë¦„': 'ëŒ€ë‘',
        'ëŒ€ë‘ë‹¨ë°±ì§ˆ': 'ëŒ€ë‘',
        'ëŒ€ë‘ë¶„ë§': 'ëŒ€ë‘',
        'ëŒ€ë‘ìœ ': 'ëŒ€ë‘',
        'ì‹ë¬¼ì„±ë‹¨ë°±ê°€ìˆ˜ë¶„í•´ë¬¼': 'ëŒ€ë‘',
        
        # ë•…ì½© ê´€ë ¨ - ë•…ì½©ìœ¼ë¡œ í†µì¼
        'ë•…ì½©': 'ë•…ì½©',
        'í”¼ë„›': 'ë•…ì½©',
        'ë•…ì½©ë²„í„°': 'ë•…ì½©',
        'ë•…ì½©ì˜¤ì¼': 'ë•…ì½©',
        'ë•…ì½©ë¶„ë§': 'ë•…ì½©',
        
        # ê²¬ê³¼ë¥˜ - ê°ê° ìœ ì§€
        'í˜¸ë‘': 'í˜¸ë‘',
        'ì£': 'ì£',
        'ì•„ëª¬ë“œ': 'ì•„ëª¬ë“œ',
        'ìºìŠˆë„›': 'ìºìŠˆë„›',
        'í”¼ìŠ¤íƒ€ì¹˜ì˜¤': 'í”¼ìŠ¤íƒ€ì¹˜ì˜¤',
        'ë§ˆì¹´ë‹¤ë¯¸ì•„': 'ë§ˆì¹´ë‹¤ë¯¸ì•„',
        
        # ë©”ë°€ ê´€ë ¨ - ë©”ë°€ë¡œ í†µì¼
        'ë©”ë°€': 'ë©”ë°€',
        'ë©”ë°€ê°€ë£¨': 'ë©”ë°€',
        'ë©”ë°€ë©´': 'ë©”ë°€',
        
        # ê°‘ê°ë¥˜ - ê°ê° ìœ ì§€
        'ìƒˆìš°': 'ìƒˆìš°',
        'ìƒˆìš°ë¶„ë§': 'ìƒˆìš°',
        'ê²Œ': 'ê²Œ',
        'ê²Œë¶„ë§': 'ê²Œ',
        'ëìŠ¤í„°': 'ëìŠ¤í„°',
        'ê°€ì¬': 'ê°€ì¬',
        
        # ì¡°ê°œë¥˜ - ì¡°ê°œë¡œ í†µì¼
        'ì¡°ê°œ': 'ì¡°ê°œ',
        'ì¡°ê°œë¥˜': 'ì¡°ê°œ',
        'êµ´': 'ì¡°ê°œ',
        'ì „ë³µ': 'ì¡°ê°œ',
        'í™í•©': 'ì¡°ê°œ',
        'ë°”ì§€ë½': 'ì¡°ê°œ',
        'ê´€ì': 'ì¡°ê°œ',
        
        # ì–´ë¥˜ - ì–´ë¥˜ë¡œ í†µì¼
        'ê³ ë“±ì–´': 'ì–´ë¥˜',
        'ì—°ì–´': 'ì–´ë¥˜',
        'ì°¸ì¹˜': 'ì–´ë¥˜',
        'ë©¸ì¹˜': 'ì–´ë¥˜',
        'ì˜¤ì§•ì–´': 'ì–´ë¥˜',
        'ë¬¸ì–´': 'ì–´ë¥˜',
        'ì–´ë¥˜': 'ì–´ë¥˜',
        
        # ìœ¡ë¥˜ - ê°ê° ìœ ì§€
        'ì‡ ê³ ê¸°': 'ì‡ ê³ ê¸°',
        'ì†Œê³ ê¸°': 'ì‡ ê³ ê¸°',
        'ë¼ì§€ê³ ê¸°': 'ë¼ì§€ê³ ê¸°',
        'ë¼ì§€': 'ë¼ì§€ê³ ê¸°',
        'ë‹­ê³ ê¸°': 'ë‹­ê³ ê¸°',
        'ì–‘ê³ ê¸°': 'ì–‘ê³ ê¸°',
        
        # ë³µìˆ­ì•„ ê´€ë ¨ - ë³µìˆ­ì•„ë¡œ í†µì¼
        'ë³µìˆ­ì•„': 'ë³µìˆ­ì•„',
        'ë³µìˆ­ì•„ì¦™': 'ë³µìˆ­ì•„',
        'ë³µìˆ­ì•„í–¥ë£Œ': 'ë³µìˆ­ì•„',
        
        # í† ë§ˆí†  ê´€ë ¨ - í† ë§ˆí† ë¡œ í†µì¼
        'í† ë§ˆí† ': 'í† ë§ˆí† ',
        'í† ë§ˆí† í˜ì´ìŠ¤íŠ¸': 'í† ë§ˆí† ',
        'í† ë§ˆí† ì†ŒìŠ¤': 'í† ë§ˆí† ',
        'í† ë§ˆí† ì¶”ì¶œë¬¼': 'í† ë§ˆí† ',
        
        # ì•„í™©ì‚°ë¥˜ ê´€ë ¨ - ì•„í™©ì‚°ë¥˜ë¡œ í†µì¼
        'ì•„í™©ì‚°ë‚˜íŠ¸ë¥¨': 'ì•„í™©ì‚°ë¥˜',
        'ì•„í™©ì‚°ì¹¼ë¥¨': 'ì•„í™©ì‚°ë¥˜',
        'ì•„í™©ì‚°ìˆ˜ì†Œë‚˜íŠ¸ë¥¨': 'ì•„í™©ì‚°ë¥˜',
        'ì•„í™©ì‚°ë¥˜': 'ì•„í™©ì‚°ë¥˜',
        
        # ê¸°íƒ€ - ê°ê° ìœ ì§€
        'ì¹´ë¼ë©œìƒ‰ì†Œ': 'ì¹´ë¼ë©œìƒ‰ì†Œ',
        'ì•„ì§ˆì‚°ë‚˜íŠ¸ë¥¨': 'ì•„ì§ˆì‚°ë‚˜íŠ¸ë¥¨',
        'ë³µí•©ì¡°ë¯¸ë£Œ': 'ë³µí•©ì¡°ë¯¸ë£Œ',
        'MSG': 'MSG',
        'ì¡°ë¯¸ë£Œ': 'ì¡°ë¯¸ë£Œ'
    }
    
    # ì¤‘ë³µ ì œê±°ëœ ì„±ë¶„ë“¤ì„ ì €ì¥í•  ì„¸íŠ¸
    unique_allergens = set()
    processed_ingredients = []
    
    for ingredient in ingredients:
        # ê° ì„±ë¶„ì—ì„œ ì•Œë ˆë¥´ê¸° ì„±ë¶„ ë§¤í•‘ ì°¾ê¸°
        found_allergen = None
        for key, mapped_allergen in allergen_mapping.items():
            if key in ingredient:
                found_allergen = mapped_allergen
                break
        
        # ë§¤í•‘ëœ ì•Œë ˆë¥´ê¸° ì„±ë¶„ì´ ìˆê³  ì•„ì§ ì¶”ê°€ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì¶”ê°€
        if found_allergen and found_allergen not in unique_allergens:
            unique_allergens.add(found_allergen)
            processed_ingredients.append(found_allergen)
    
    return processed_ingredients

def filter_allergen_ingredients(ingredients):
    """
    ì¶”ì¶œëœ ì¬ë£Œëª… ì¤‘ì—ì„œ ì•Œë ˆë¥´ê¸° ìœ ë°œ ì„±ë¶„ë§Œ í•„í„°ë§í•˜ê³  ì¤‘ë³µ ì œê±°í•˜ëŠ” í•¨ìˆ˜
    """
    # ì•Œë ˆë¥´ê¸° ìœ ë°œ ì„±ë¶„ ëª©ë¡ (ì‹ì•½ì²˜ ì§€ì • 23ì¢… + ê´€ë ¨ ì¬ë£Œ)
    allergen_list = [
        # ê³¡ë¬¼ë¥˜ (ë°€)
        'ë°€ê°€ë£¨', 'ë°€', 'ë°€ê¸€ë£¨í…', 'ë°€ì „ë¶„', 'ë°€íš¨ì†Œ', 'ë°€ë‹¨ë°±ì§ˆ',
        
        # ë‚œë¥˜ (ë‹¬ê±€)
        'ë‹¬ê±€', 'ê³„ë€', 'ì „ë€ì•¡', 'ë‚œí™©ì•¡', 'ë‚œë°±ë¶„', 'ë‚œí™©ë¶„ë§', 'ë‚œë°±ë¶„ë§',
        
        # ìš°ìœ 
        'ìš°ìœ ', 'ì „ì§€ë¶„ìœ ', 'íƒˆì§€ë¶„ìœ ', 'ìœ í¬ë¦¼', 'ê°€ê³µìœ í¬ë¦¼', 'ìœ ë‹¹', 'ìœ ì²­ë‹¨ë°±ë¶„ë§',
        'í˜¼í•©ë¶„ìœ ', 'ì—°ìœ ', 'ë²„í„°', 'ë§ˆê°€ë¦°', 'ì¹˜ì¦ˆ', 'ìš”ê±°íŠ¸',
        
        # ëŒ€ë‘
        'ëŒ€ë‘', 'ì½©', 'ë‘ë¶€', 'ëœì¥', 'ê°„ì¥', 'ê³ ì¶”ì¥', 'ì½©ê¸°ë¦„', 'ëŒ€ë‘ë‹¨ë°±ì§ˆ',
        'ëŒ€ë‘ë¶„ë§', 'ëŒ€ë‘ìœ ', 'ì‹ë¬¼ì„±ë‹¨ë°±ê°€ìˆ˜ë¶„í•´ë¬¼',
        
        # ë•…ì½©
        'ë•…ì½©', 'í”¼ë„›', 'ë•…ì½©ë²„í„°', 'ë•…ì½©ì˜¤ì¼', 'ë•…ì½©ë¶„ë§',
        
        # ê²¬ê³¼ë¥˜
        'í˜¸ë‘', 'ì£', 'ì•„ëª¬ë“œ', 'ìºìŠˆë„›', 'í”¼ìŠ¤íƒ€ì¹˜ì˜¤', 'ë§ˆì¹´ë‹¤ë¯¸ì•„',
        
        # ë©”ë°€
        'ë©”ë°€', 'ë©”ë°€ê°€ë£¨', 'ë©”ë°€ë©´',
        
        # ê°‘ê°ë¥˜
        'ìƒˆìš°', 'ê²Œ', 'ëìŠ¤í„°', 'ê°€ì¬', 'ìƒˆìš°ë¶„ë§', 'ê²Œë¶„ë§',
        
        # ì¡°ê°œë¥˜
        'ì¡°ê°œ', 'êµ´', 'ì „ë³µ', 'í™í•©', 'ë°”ì§€ë½', 'ê´€ì', 'ì¡°ê°œë¥˜',
        
        # ì–´ë¥˜
        'ê³ ë“±ì–´', 'ì—°ì–´', 'ì°¸ì¹˜', 'ë©¸ì¹˜', 'ì˜¤ì§•ì–´', 'ë¬¸ì–´', 'ì–´ë¥˜',
        
        # ìœ¡ë¥˜
        'ì‡ ê³ ê¸°', 'ë¼ì§€ê³ ê¸°', 'ë‹­ê³ ê¸°', 'ì–‘ê³ ê¸°', 'ì†Œê³ ê¸°', 'ë¼ì§€',
        
        # ë³µìˆ­ì•„
        'ë³µìˆ­ì•„', 'ë³µìˆ­ì•„ì¦™', 'ë³µìˆ­ì•„í–¥ë£Œ',
        
        # í† ë§ˆí† 
        'í† ë§ˆí† ', 'í† ë§ˆí† í˜ì´ìŠ¤íŠ¸', 'í† ë§ˆí† ì†ŒìŠ¤', 'í† ë§ˆí† ì¶”ì¶œë¬¼',
        
        # ì•„í™©ì‚°ë¥˜
        'ì•„í™©ì‚°ë‚˜íŠ¸ë¥¨', 'ì•„í™©ì‚°ì¹¼ë¥¨', 'ì•„í™©ì‚°ìˆ˜ì†Œë‚˜íŠ¸ë¥¨', 'ì•„í™©ì‚°ë¥˜',
        
        # ê¸°íƒ€ ì•Œë ˆë¥´ê¸° ê´€ë ¨
        'ì¹´ë¼ë©œìƒ‰ì†Œ', 'ì•„ì§ˆì‚°ë‚˜íŠ¸ë¥¨', 'ë³µí•©ì¡°ë¯¸ë£Œ', 'MSG', 'ì¡°ë¯¸ë£Œ'
    ]
    
    # ì•Œë ˆë¥´ê¸° ìœ ë°œ ì„±ë¶„ë§Œ í•„í„°ë§
    filtered_ingredients = []
    
    for ingredient in ingredients:
        # ê¸°ë³¸ ì¬ë£Œëª…ì—ì„œ ì•Œë ˆë¥´ê¸° ì„±ë¶„ í™•ì¸
        for allergen in allergen_list:
            if allergen in ingredient:
                filtered_ingredients.append(ingredient)
                break
        
        # ê´„í˜¸ ì•ˆì˜ ì›ì‚°ì§€ ì •ë³´ì—ì„œë„ ì•Œë ˆë¥´ê¸° ì„±ë¶„ í™•ì¸
        if '(' in ingredient and ')' in ingredient:
            origin_text = ingredient[ingredient.find('(')+1:ingredient.find(')')]
            for allergen in allergen_list:
                if allergen in origin_text:
                    filtered_ingredients.append(ingredient)
                    break
    
    # ì¤‘ë³µ ì œê±° ì ìš©
    unique_ingredients = remove_duplicate_allergens(filtered_ingredients)
    
    return unique_ingredients

def extract_from_full_text(text):
    """
    ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì›ì¬ë£Œëª… íŒ¨í„´ì„ ì°¾ëŠ” í•¨ìˆ˜
    """
    ingredients = []
    
    # ì›ì¬ë£Œëª… ê´€ë ¨ í‚¤ì›Œë“œ ì£¼ë³€ì—ì„œ ì¶”ì¶œ
    ingredient_keywords = [
        r'ì›ì¬ë£Œëª…?\s*[:ï¼š]\s*([^ê°€-í£]*[ê°€-í£]+[^ê°€-í£]*(?:[ê°€-í£]+[^ê°€-í£]*)*)',
        r'ì„±ë¶„\s*[:ï¼š]\s*([^ê°€-í£]*[ê°€-í£]+[^ê°€-í£]*(?:[ê°€-í£]+[^ê°€-í£]*)*)',
    ]
    
    for pattern in ingredient_keywords:
        matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
        for match in matches:
            individual_ingredients = split_compound_ingredient(match)
            for ingredient in individual_ingredients:
                cleaned = clean_ingredient_text(ingredient)
                if cleaned and is_valid_ingredient(cleaned):
                    ingredients.append(cleaned)
    
    return ingredients

def split_compound_ingredient(ingredient_text):
    """
    ë³µí•© ì¬ë£Œëª…ì„ ê°œë³„ ì¬ë£Œë¡œ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    ingredients = []
    
    # ì‰¼í‘œë¡œ ë¶„ë¦¬
    parts = re.split(r'[,ï¼Œ]', ingredient_text)
    
    for part in parts:
        part = part.strip()
        if part:
            # ê´„í˜¸ ì•ˆì˜ ì›ì‚°ì§€ ì •ë³´ëŠ” ì œê±°í•˜ê³  ê¸°ë³¸ ì¬ë£Œëª…ë§Œ ì¶”ì¶œ
            base_ingredient = re.sub(r'\([^)]*\)', '', part).strip()
            if base_ingredient:
                ingredients.append(base_ingredient)
            
            # ê´„í˜¸ ì•ˆì˜ ì›ì‚°ì§€ ì •ë³´ë„ ë³„ë„ë¡œ ì €ì¥ (ì˜ˆ: ë°€ê°€ë£¨(ë°€:ë¯¸êµ­ì‚°)ì—ì„œ "ë°€")
            origin_match = re.search(r'\(([^)]*)\)', part)
            if origin_match:
                origin_text = origin_match.group(1)
                # ì›ì‚°ì§€ì—ì„œ ì‹¤ì œ ì¬ë£Œëª… ì¶”ì¶œ (ì˜ˆ: "ë°€:ë¯¸êµ­ì‚°"ì—ì„œ "ë°€")
                origin_ingredient = re.search(r'([ê°€-í£]+)', origin_text)
                if origin_ingredient:
                    ingredients.append(origin_ingredient.group(1))
    
    return ingredients

def extract_direct_patterns(text):
    """
    ì§ì ‘ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì¬ë£Œëª…ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    ingredients = []
    
    # ì•Œë ˆë¥´ê¸° ìœ ë°œ ì„±ë¶„ë“¤ (ì‹ì•½ì²˜ ì§€ì • 23ì¢… + ê´€ë ¨ ì¬ë£Œ)
    known_ingredients = [
        # ê³¡ë¬¼ë¥˜ (ë°€)
        'ë°€ê°€ë£¨', 'ë°€', 'ë°€ê¸€ë£¨í…', 'ë°€ì „ë¶„', 'ë°€íš¨ì†Œ', 'ë°€ë‹¨ë°±ì§ˆ',
        
        # ë‚œë¥˜ (ë‹¬ê±€)
        'ë‹¬ê±€', 'ê³„ë€', 'ì „ë€ì•¡', 'ë‚œí™©ì•¡', 'ë‚œë°±ë¶„', 'ë‚œí™©ë¶„ë§', 'ë‚œë°±ë¶„ë§',
        
        # ìš°ìœ 
        'ìš°ìœ ', 'ì „ì§€ë¶„ìœ ', 'íƒˆì§€ë¶„ìœ ', 'ìœ í¬ë¦¼', 'ê°€ê³µìœ í¬ë¦¼', 'ìœ ë‹¹', 'ìœ ì²­ë‹¨ë°±ë¶„ë§',
        'í˜¼í•©ë¶„ìœ ', 'ì—°ìœ ', 'ë²„í„°', 'ë§ˆê°€ë¦°', 'ì¹˜ì¦ˆ', 'ìš”ê±°íŠ¸',
        
        # ëŒ€ë‘
        'ëŒ€ë‘', 'ì½©', 'ë‘ë¶€', 'ëœì¥', 'ê°„ì¥', 'ê³ ì¶”ì¥', 'ì½©ê¸°ë¦„', 'ëŒ€ë‘ë‹¨ë°±ì§ˆ',
        'ëŒ€ë‘ë¶„ë§', 'ëŒ€ë‘ìœ ', 'ì‹ë¬¼ì„±ë‹¨ë°±ê°€ìˆ˜ë¶„í•´ë¬¼',
        
        # ë•…ì½©
        'ë•…ì½©', 'í”¼ë„›', 'ë•…ì½©ë²„í„°', 'ë•…ì½©ì˜¤ì¼', 'ë•…ì½©ë¶„ë§',
        
        # ê²¬ê³¼ë¥˜
        'í˜¸ë‘', 'ì£', 'ì•„ëª¬ë“œ', 'ìºìŠˆë„›', 'í”¼ìŠ¤íƒ€ì¹˜ì˜¤', 'ë§ˆì¹´ë‹¤ë¯¸ì•„',
        
        # ë©”ë°€
        'ë©”ë°€', 'ë©”ë°€ê°€ë£¨', 'ë©”ë°€ë©´',
        
        # ê°‘ê°ë¥˜
        'ìƒˆìš°', 'ê²Œ', 'ëìŠ¤í„°', 'ê°€ì¬', 'ìƒˆìš°ë¶„ë§', 'ê²Œë¶„ë§',
        
        # ì¡°ê°œë¥˜
        'ì¡°ê°œ', 'êµ´', 'ì „ë³µ', 'í™í•©', 'ë°”ì§€ë½', 'ê´€ì', 'ì¡°ê°œë¥˜',
        
        # ì–´ë¥˜
        'ê³ ë“±ì–´', 'ì—°ì–´', 'ì°¸ì¹˜', 'ë©¸ì¹˜', 'ì˜¤ì§•ì–´', 'ë¬¸ì–´', 'ì–´ë¥˜',
        
        # ìœ¡ë¥˜
        'ì‡ ê³ ê¸°', 'ë¼ì§€ê³ ê¸°', 'ë‹­ê³ ê¸°', 'ì–‘ê³ ê¸°', 'ì†Œê³ ê¸°', 'ë¼ì§€',
        
        # ë³µìˆ­ì•„
        'ë³µìˆ­ì•„', 'ë³µìˆ­ì•„ì¦™', 'ë³µìˆ­ì•„í–¥ë£Œ',
        
        # í† ë§ˆí† 
        'í† ë§ˆí† ', 'í† ë§ˆí† í˜ì´ìŠ¤íŠ¸', 'í† ë§ˆí† ì†ŒìŠ¤', 'í† ë§ˆí† ì¶”ì¶œë¬¼',
        
        # ì•„í™©ì‚°ë¥˜
        'ì•„í™©ì‚°ë‚˜íŠ¸ë¥¨', 'ì•„í™©ì‚°ì¹¼ë¥¨', 'ì•„í™©ì‚°ìˆ˜ì†Œë‚˜íŠ¸ë¥¨', 'ì•„í™©ì‚°ë¥˜',
        
        # ê¸°íƒ€ ì•Œë ˆë¥´ê¸° ê´€ë ¨
        'ì¹´ë¼ë©œìƒ‰ì†Œ', 'ì•„ì§ˆì‚°ë‚˜íŠ¸ë¥¨', 'ë³µí•©ì¡°ë¯¸ë£Œ', 'MSG', 'ì¡°ë¯¸ë£Œ'
    ]
    
    for ingredient in known_ingredients:
        if ingredient in text:
            ingredients.append(ingredient)
    
    # ë³µí•© íŒ¨í„´ ê²€ìƒ‰ (ê´„í˜¸ë‚˜ íŠ¹ìˆ˜ë¬¸ì í¬í•¨)
    complex_patterns = [
        r'([ê°€-í£]+\([^)]*\))',  # ê´„í˜¸ í¬í•¨ ì¬ë£Œëª…
        r'([ê°€-í£]+\[[^\]]*\])',  # ëŒ€ê´„í˜¸ í¬í•¨ ì¬ë£Œëª…
        r'([ê°€-í£]+(?:ê°€ë£¨|ë¶„ë§|ìœ ì§€|ì˜¤ì¼|ìœ |ì œ|ë£Œ|ë¶„ìœ |ë§¤ìŠ¤|íƒ•|ë‹|ë¥˜|í’ˆ|ì•¡|í¬ë¦¼|ì‹œëŸ½|ì¶”ì¶œë¬¼|ë¶„í•´ë¬¼|ë¯¹ìŠ¤))',  # ì ‘ë¯¸ì‚¬ íŒ¨í„´
    ]
    
    for pattern in complex_patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            cleaned = clean_ingredient_text(match)
            if cleaned and is_valid_ingredient(cleaned):
                ingredients.append(cleaned)
    
    return ingredients

def find_ingredient_section(text):
    """
    ì›ì¬ë£Œëª… ì„¹ì…˜ì„ ì°¾ëŠ” í•¨ìˆ˜ (ê°œì„ ëœ ë²„ì „)
    """
    # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ì›ì¬ë£Œëª… ì„¹ì…˜ ì°¾ê¸°
    patterns = [
        # ê¸°ë³¸ íŒ¨í„´
        r'ì›ì¬ë£Œëª…?\s*[:ï¼š]\s*([\s\S]*?)(?=\n\n|\n[ê°€-í£]+:|$)',
        # "ì›ì¬ë£Œ:" íŒ¨í„´
        r'ì›ì¬ë£Œ\s*[:ï¼š]\s*([\s\S]*?)(?=\n\n|\n[ê°€-í£]+:|$)',
        # ë” ìœ ì—°í•œ íŒ¨í„´
        r'ì›ì¬ë£Œëª…?\s*[:ï¼š]\s*([^ê°€-í£]*[ê°€-í£]+[^ê°€-í£]*(?:[ê°€-í£]+[^ê°€-í£]*)*)',
        # ì„±ë¶„ ê´€ë ¨ íŒ¨í„´
        r'ì„±ë¶„\s*[:ï¼š]\s*([\s\S]*?)(?=\n\n|\n[ê°€-í£]+:|$)',
    ]
    
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
        if match:
            section = match.group(1).strip()
            # ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
            if len(section) > 10:
                return section
    
    # ì›ì¬ë£Œëª… í‚¤ì›Œë“œê°€ ì—†ëŠ” ê²½ìš°, í…ìŠ¤íŠ¸ì—ì„œ ì„±ë¶„ ê´€ë ¨ ë‚´ìš© ì°¾ê¸°
    # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ê¸´ í…ìŠ¤íŠ¸ ë¸”ë¡ ì°¾ê¸°
    long_text_pattern = r'([ê°€-í£]+(?:\([^)]*\))?(?:,|\s*,\s*)[ê°€-í£]+(?:\([^)]*\))?(?:,|\s*,\s*)[ê°€-í£]+(?:\([^)]*\))?(?:,|\s*,\s*)[ê°€-í£]+(?:\([^)]*\))?)'
    match = re.search(long_text_pattern, text, re.MULTILINE)
    
    if match:
        return match.group(1).strip()
    
    return None

def is_valid_ingredient(text):
    """
    ìœ íš¨í•œ ì¬ë£Œëª…ì¸ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜ (í•œêµ­ ì‹í’ˆ ì„±ë¶„í‘œ íŠ¹í™”)
    """
    if not text or len(text) < 2:
        return False
    
    # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ì œì™¸ (30ì ì´ìƒ)
    if len(text) > 30:
        return False
    
    # í•œêµ­ì–´ ë¹„ìœ¨ í™•ì¸ (50% ì´ìƒ)
    korean_chars = len(re.findall(r'[ê°€-í£]', text))
    if korean_chars < len(text) * 0.5:
        return False
    
    # ì œì™¸í•  ë‹¨ì–´ë“¤ (ë” êµ¬ì²´ì ìœ¼ë¡œ)
    exclude_words = [
        # ì œí’ˆ ì •ë³´
        'ì œí’ˆëª…', 'ì‹í’ˆìœ í˜•', 'ì†Œë¹„ê¸°í•œ', 'í’ˆëª©ë³´ê³ ë²ˆí˜¸', 'í¬ì¥ì¬ì§ˆ', 'ì—…ì†Œëª…', 'ì†Œì¬ì§€',
        'ì¸¡ë©´í‘œê¸°ì¼ê¹Œì§€', 'ì¦‰ë©´í‘œê¸°ì¼ê¹Œì§€', 'ì œì¡°ì›', 'ëŒ€ë¥™ì‹í’ˆ', 'ë•ê³„ê³µì¥',
        
        # ì£¼ì†Œ/ìœ„ì¹˜ ì •ë³´
        'ì„œìš¸ì‹œ', 'ì˜ë“±í¬êµ¬', 'ê²½ë‚¨', 'ì–‘ì‚°ì‹œ', 'ê·¸ë¦°ê³µë‹¨', 'ë§ë ˆì´ì‹œì•„ì‚°', 'ë¯¸êµ­ì‚°', 'ì‹±ê°€í¬ë¥´ì‚°',
        
        # ì•ˆì „/ë³´ê´€ ì •ë³´
        'ì§ì‚¬ê´‘ì„ ', 'ìŠµê¸°', 'í”¼í•´', 'ë³´ê´€', 'ê°œë´‰', 'í›„', 'ê°€ê¸‰ì ', 'ë¹¨ë¦¬', 'ë“œì„¸ìš”',
        'ë¶€ì •ë¶ˆëŸ‰ì‹í’ˆ', 'ì‹ ê³ ', 'êµ­ë²ˆì—†ì´', '1399', 'ê³ ê°ì„¼í„°', 'ì „í™”', 'ë¬¸ì', 'ë°˜í’ˆì²˜',
        'ë³¸ì‚¬', 'êµ¬ì…í•œ', 'ê³µê¸°ì£¼ì…', 'ë°©ì‹ì„', 'ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤', 'ì†Œë¹„ì', 'ê¸°ë³¸ë²•', 'ì˜í•œ', 'í”¼í•´ë³´ìƒ',
        
        # ì˜ì–‘ì •ë³´
        'ê¸°ì¤€ì¹˜ì—', 'ëŒ€í•œ', 'ë‚´ìš©ëŸ‰ë‹¹', 'ì½œë ˆìŠ¤í…Œë¡¤', 'í¬í™”ì§€ë°©', 'ì´', 'ê°œ', 'í•¨ëŸ‰', 'ê¸°íƒ€',
        
        # ì¼ë°˜ì ì¸ ì„¤ëª…
        'ìƒê¸¸', 'ìˆìœ¼ë‚˜', 'ì¸ì²´ì—', 'ë¬´í•´í•˜ë‹ˆ', 'ë“œì…”ë„', 'ê´œì°®ìŠµë‹ˆë‹¤', 'ë‚˜ëˆ„ì–´', 'ì¡°ê¸ˆì”©',
        'í•˜ì•ˆ', 'ì£¼', 'ì£¼ ë‚´ìš©ëŸ‰ë‹¹',
        
        # OCR ì˜¤ë¥˜ë¡œ ì¸í•œ ì˜ëª»ëœ ì¶”ì¶œ
        'ë¶ˆëŸ‰ì‹í’ˆ', 'í´ë¦¬í”„ë¡œí•„ë Œ ì†Œë¹„ê¸°í•œ', 'ë³€ì§ˆí’ˆ'
    ]
    
    # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë§Œ ì œì™¸
    for word in exclude_words:
        if text == word or text.startswith(word + '(') or text.endswith(')' + word):
            return False
    
    # ì•Œë ˆë¥´ê¸° ê´€ë ¨ ë¬¸êµ¬ê°€ í¬í•¨ëœ ê²½ìš° ì œì™¸
    if any(phrase in text for phrase in ['í•¨ìœ ', 'í˜¼ì… ê°€ëŠ¥', 'í¬í•¨', 'í˜¼ì…ê°€ëŠ¥']):
        return False
    
    # ì‹¤ì œ ì‹í’ˆ ì›ì¬ë£Œëª… íŒ¨í„´ í™•ì¸ (í™•ì¥ëœ ëª©ë¡)
    valid_ingredients = [
        # ê³¡ë¬¼ë¥˜
        'ë°€ê°€ë£¨', 'ë°€', 'ì˜¥ìˆ˜ìˆ˜', 'ìŒ€', 'ë³´ë¦¬', 'ê·€ë¦¬',
        
        # ë‹¹ë¥˜
        'ì„¤íƒ•', 'ë°±ì„¤íƒ•', 'ë‹¹ë¥˜', 'ë‹¹ë¥˜ê°€ê³µí’ˆ', 'í¬ë„ë‹¹', 'ê³¼ë‹¹', 'ìœ ë‹¹',
        
        # ìœ ì§€ë¥˜
        'ì‡¼íŠ¸ë‹', 'ê°€ê³µìœ ì§€', 'ì‹ë¬¼ì„±ìœ ì§€', 'íŒœìœ ', 'íŒ¡ìœ ', 'íŒ¡ì—ìŠ¤í…Œë¥´í™”ìœ ',
        'í•´ë°”ë¼ê¸°ìœ ', 'ì˜¬ë ˆì˜¤ë ˆì§„ë¡œì¦ˆë©”ë¦¬', 'ë²„í„°', 'ë§ˆê°€ë¦°',
        
        # ìœ ì œí’ˆ
        'ì „ì§€ë¶„ìœ ', 'íƒˆì§€ë¶„ìœ ', 'ìœ ì²­ë‹¨ë°±ë¶„ë§', 'ìš°ìœ ', 'ìœ í¬ë¦¼', 'ê°€ê³µìœ í¬ë¦¼',
        'í˜¼í•©ë¶„ìœ ', 'ì—°ìœ ', 'ì¹´ë¼ê¸°ë‚œ',
        
        # ì½”ì½”ì•„/ì´ˆì½œë¦¿
        'ì½”ì½”ì•„ë§¤ìŠ¤', 'ì½”ì½”ì•„í”„ë¦¬í¼ë ˆì´ì…˜', 'ê³ ì½”ì•„ë§¤ìŠ¤', 'ì „ì§€ë¶„ê³¨ë“œ',
        
        # ì²¨ê°€ë¬¼
        'ì‚°ë„ì¡°ì ˆì œ', 'ìœ í™”ì œ', 'í˜¼í•©ì œì œ', 'ë¶ˆí™œì„±ê±´ì¡°íš¨ëª¨', 'íš¨ëª¨', 'ìƒì´ìŠ¤íŠ¸',
        'í•©ì„±íŒ½ì°½ì œ', 'íƒ„ì‚°ìˆ˜ì†Œë‚˜íŠ¸ë¥¨', 'ì‚°ì„±í”¼ë¡œì¸ì‚°ë‚˜íŠ¸ë¥¨', 'ì œì¼ì¸ì‚°ì¹¼ìŠ˜', 'ì –ì‚°ì¹¼ìŠ˜',
        'ìŠ¤í…Œì•„ë¦´ì –ì‚°ë‚˜íŠ¸ë¥¨', 'ë¹„íƒ€ë¯¼C', 'Î±-ì•„ë°€ë¼ì•„ì œ', 'ìì¼ë¼ë‚˜ì•„ì œ',
        'ì´ˆì‚°ë‚˜íŠ¸ë¥¨', 'ì´ˆì‚°', 'ë¹„íƒ€ë¯¼Bâ‚ë¼ìš°ë¦´í™©ì‚°ì—¼',
        
        # í–¥ë£Œ/í–¥ì‹ ë£Œ
        'í–¥ë£Œ', 'ë°”ë‹ë¼', 'ë°”ë‹ë¼ì¶”ì¶œë¬¼', 'ë°”ë‹ë¼í¬ë¦¼', 'ë°”ë‹ë¦°', 'íš¨ì†Œì œ', 'êµ¬ë©´ì‹ ',
        'ì„¤íƒ•ì‹œëŸ½', 'í”ŒëŸ¬ìŠ¤ë¡œìŠ¤íŠ¸ì½˜ë§›ì”¨ì¦ˆë‹', 'ê°„ì¥ë¶„ë§', 'ì§„ê°„ì¥', 'ë³µí•©ì¡°ë¯¸ì‹í’ˆ',
        
        # ê¸°íƒ€
        'ì •ì œì†Œê¸ˆ', 'ì „ë€ì•¡', 'ë‚œí™©ì•¡', 'ë‚œë°±ë¶„', 'ìŠˆê°€íŒŒìš°ë”', 'ê¸°íƒ€ê°€ê³µí’ˆ',
        'ë³€ì„±ì „ë¶„', 'ì˜¥ìˆ˜ìˆ˜ì „ë¶„', 'ì‹ë¬¼ì„±ë‹¨ë°±ê°€ìˆ˜ë¶„í•´ë¬¼', 'ëŒ€ë‘',
        'ë…¸í‹°ë“œë„ë„›ë¯¹ìŠ¤', 'ì˜ì–‘ê°•í™”ë°€ê°€ë£¨', 'ë°€ê¸€ë£¨í…', 'í—¤ë¯¸ì…€ë£°ë¼ì•„ì œ', 'í™©ì‚°ì¹¼ìŠ˜'
    ]
    
    # ì‹¤ì œ ì¬ë£Œëª…ê³¼ ìœ ì‚¬í•œ íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸
    for ingredient in valid_ingredients:
        if ingredient in text:
            return True
    
    # ì¼ë°˜ì ì¸ ì‹í’ˆ ê´€ë ¨ íŒ¨í„´ (í™•ì¥ëœ íŒ¨í„´)
    food_patterns = [
        r'.*ê°€ë£¨$', r'.*ë¶„ë§$', r'.*ìœ ì§€$', r'.*ì˜¤ì¼$', r'.*ìœ $', r'.*ì œ$', r'.*ë£Œ$',
        r'.*ë¶„ìœ $', r'.*ë§¤ìŠ¤$', r'.*íƒ•$', r'.*ë‹$', r'.*ë¥˜$', r'.*í’ˆ$', r'.*ì•¡$',
        r'.*í¬ë¦¼$', r'.*ì‹œëŸ½$', r'.*ì¶”ì¶œë¬¼$', r'.*ë¶„í•´ë¬¼$', r'.*ë¯¹ìŠ¤$', r'.*ê°•í™”.*$'
    ]
    
    for pattern in food_patterns:
        if re.match(pattern, text):
            return True
    
    return False

def clean_ingredient_text(text):
    """
    ì¶”ì¶œëœ ì¬ë£Œëª… í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•˜ëŠ” í•¨ìˆ˜ (í•œêµ­ ì‹í’ˆ ì„±ë¶„í‘œ íŠ¹í™” - ê°œì„ ëœ ë²„ì „)
    """
    if not text:
        return ""
    
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    
        # OCR ì˜¤ë¥˜ë¡œ ì¸í•œ ì˜ëª»ëœ ë¬¸ìë“¤ ìˆ˜ì • (ë” í¬ê´„ì ìœ¼ë¡œ)
    ocr_corrections = {
        # ì¼ë°˜ì ì¸ OCR ì˜¤ë¥˜
        '0': 'O', '1': 'l', '2': 'Z', '5': 'S', '8': 'B',
        
        # í•œêµ­ì–´ ì¬ë£Œëª… OCR ì˜¤ë¥˜ ìˆ˜ì •
        'ì„¤ íƒ•': 'ì„¤íƒ•', 'ì •ì œì†Œ ê¸ˆ': 'ì •ì œì†Œê¸ˆ', 'ì‚°ë„ì¡°ì ˆ ì œ': 'ì‚°ë„ì¡°ì ˆì œ',
        'ìœ í™” ì œ': 'ìœ í™”ì œ', 'í˜¼í•© ì œì œ': 'í˜¼í•©ì œì œ', 'ë°”ë‹ ë¼': 'ë°”ë‹ë¼',
        'ì „ì§€ë¶„ ìœ ': 'ì „ì§€ë¶„ìœ ', 'ìœ ì²­ë‹¨ë°±ë¶„ ë§': 'ìœ ì²­ë‹¨ë°±ë¶„ë§',
        'ë¶ˆí™œì„±ê±´ì¡°íš¨ ëª¨': 'ë¶ˆí™œì„±ê±´ì¡°íš¨ëª¨', 'ê³ ì½”ì•„ë§¤ ìŠ¤': 'ê³ ì½”ì•„ë§¤ìŠ¤',
        'ì½”ì½”ì•„í”„ë¦¬í¼ë ˆì´ ì…˜': 'ì½”ì½”ì•„í”„ë¦¬í¼ë ˆì´ì…˜', 'íŒ¡ì—ìŠ¤í…Œë¥´í™” ìœ ': 'íŒ¡ì—ìŠ¤í…Œë¥´í™”ìœ ',
        
        # ê³µë°± ì œê±°ê°€ í•„ìš”í•œ ê²½ìš°ë“¤
        'ë°€ ê°€ë£¨': 'ë°€ê°€ë£¨', 'ë§ˆê°€ ë¦°': 'ë§ˆê°€ë¦°', 'íš¨ ëª¨': 'íš¨ëª¨',
        'ì „ë€ ì•¡': 'ì „ë€ì•¡', 'ë‚œí™© ì•¡': 'ë‚œí™©ì•¡', 'í˜¼í•©ë¶„ ìœ ': 'í˜¼í•©ë¶„ìœ ',
        'ìƒì´ ìŠ¤íŠ¸': 'ìƒì´ìŠ¤íŠ¸', 'ìŠˆê°€íŒŒìš° ë”': 'ìŠˆê°€íŒŒìš°ë”',
        
        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
        'Â·': ' ', 'ï¼š': ':', 'ï¼Œ': ',',
        
        # ì¶”ê°€ OCR ì˜¤ë¥˜ ìˆ˜ì •
        'ì„¤íƒ•': 'ì„¤íƒ•',  # ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•íƒœ
    }
    
    # OCR ì˜¤ë¥˜ ìˆ˜ì • ì ìš©
    for wrong, correct in ocr_corrections.items():
        text = text.replace(wrong, correct)
    
    # ë¶ˆí•„ìš”í•œ ë¬¸ìë“¤ ì œê±° (ê´„í˜¸ëŠ” ì œê±°í•˜ì§€ ì•ŠìŒ - ì›ì‚°ì§€ ì •ë³´ í¬í•¨)
    unwanted_chars = ['[', ']', ';', 'ï¼›', '|', '\\', '/', 'Â·', 'Â·']
    for char in unwanted_chars:
        text = text.replace(char, ' ')
    
    # ë°±ë¶„ìœ¨ ì •ë³´ ì •ë¦¬ (ì˜ˆ: "ê°€ê³µìœ í¬ë¦¼31.86%" â†’ "ê°€ê³µìœ í¬ë¦¼")
    text = re.sub(r'\d+\.?\d*%', '', text)
    
    # ìˆ«ìì™€ íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (ì¬ë£Œëª…ì— í¬í•¨ëœ ìˆ«ì ì œê±°í•˜ë˜, ì¬ë£Œëª… ìì²´ëŠ” ë³´ì¡´)
    # ë‹¨, ìˆ«ìë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë¶€ë¶„ì€ ì œê±°
    text = re.sub(r'\b\d+\b', '', text)
    
    # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
    text = re.sub(r'\s+', ' ', text)
    
    # ì•ë’¤ ê³µë°± ë‹¤ì‹œ ì œê±°
    text = text.strip()
    
    # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸ (1ê¸€ì)
    if len(text) < 2:
        return ""
    
    # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ ì œì™¸ (50ì)
    if len(text) > 50:
        return ""
    
    return text

def extract_ingredients_from_image(image_path_or_object, use_easyocr=True, fast_mode=False):
    """
    ì´ë¯¸ì§€ì—ì„œ ì„±ë¶„í‘œë¥¼ ì¸ì‹í•˜ê³  ì›ì¬ë£Œëª…ë§Œ ì¶”ì¶œí•˜ëŠ” ë©”ì¸ í•¨ìˆ˜ (ì—…ê·¸ë ˆì´ë“œ ë²„ì „)
    use_easyocr: EasyOCR ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    fast_mode: ë¹ ë¥¸ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
    """
    try:
        start_time = time.time()
        
        if use_easyocr:
            # ğŸš€ EasyOCR ì‚¬ìš© (ê³ ì„±ëŠ¥)
            print("ğŸ”¥ EasyOCR ì—”ì§„ ì‚¬ìš© ì¤‘...")
            extracted_text = ocr_with_enhanced_preprocessing(image_path_or_object, use_easyocr=True)
            engine_name = "EasyOCR"
        else:
            # ğŸ“œ Tesseract ì‚¬ìš© (ê¸°ì¡´)
            print("ğŸ“œ Tesseract OCR ì—”ì§„ ì‚¬ìš© ì¤‘...")
            extracted_text = ocr_image_with_opencv(image_path_or_object, "kor+eng", fast_mode)
            engine_name = "Tesseract"
        
        # ì›ì¬ë£Œëª… ì¶”ì¶œ
        ingredients = extract_ingredients_from_text(extracted_text)
        
        end_time = time.time()
        processing_time = round(end_time - start_time, 2)
        
        return {
            'full_text': extracted_text,
            'ingredients': ingredients,
            'ingredient_count': len(ingredients),
            'processing_time': processing_time,
            'engine': engine_name,
            'mode': 'fast' if fast_mode else 'precise'
        }
    except Exception as e:
        return {
            'error': str(e),
            'full_text': '',
            'ingredients': [],
            'ingredient_count': 0,
            'processing_time': 0,
            'engine': 'Error',
            'mode': 'fast' if fast_mode else 'precise'
        }

# -------------------------------
# í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰ ì‹œ
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python ocr_test.py [ì´ë¯¸ì§€íŒŒì¼ê²½ë¡œ] [--tesseract]")
        print("  --tesseract: Tesseract OCR ì‚¬ìš© (ê¸°ë³¸ê°’: EasyOCR)")
        sys.exit(1)

    # ëª…ë ¹í–‰ ì¸ìˆ˜ ì²˜ë¦¬
    use_easyocr = True
    image_path = '/Users/oseli/Desktop/Cursor AI/data/ê³¼ì:ë¹µë¥˜/ì˜¤í”„ë¼ì¸ ë°ì´í„°/ë¹ ë‹¤ì½”ì½”ë„›.jpeg'
    
    if len(sys.argv) > 1:
        if '--tesseract' in sys.argv:
            use_easyocr = False
            print("ğŸ“œ Tesseract OCR ëª¨ë“œ ì„ íƒë¨")
        else:
            image_path = sys.argv[1]
    
    try:
        print("ğŸš€ OCR ì—”ì§„ ì—…ê·¸ë ˆì´ë“œ ë²„ì „ í…ŒìŠ¤íŠ¸ ì‹œì‘!")
        print(f"ğŸ“ ì´ë¯¸ì§€: {image_path}")
        print(f"ğŸ”§ ì—”ì§„: {'EasyOCR' if use_easyocr else 'Tesseract'}")
        print("-" * 60)
        
        # GPU ê°€ìš©ì„± í™•ì¸
        gpu_available = check_gpu_availability()
        print("-" * 60)
        
        # ì›ì¬ë£Œëª… ì¶”ì¶œ ì‹¤í–‰ (EasyOCR ìš°ì„ )
        result = extract_ingredients_from_image(image_path, use_easyocr=use_easyocr, fast_mode=False)
        
        if 'error' in result:
            print("âŒ ì˜¤ë¥˜ ë°œìƒ:", result['error'])
        else:
            print("\n=== ğŸ¯ ì„±ë¶„í‘œ ë¶„ì„ ê²°ê³¼ ===")
            print(f"âš¡ ì²˜ë¦¬ ì‹œê°„: {result['processing_time']}ì´ˆ")
            print(f"ğŸ”§ ì‚¬ìš© ì—”ì§„: {result['engine']}")
            print(f"ğŸ“Š ëª¨ë“œ: {result['mode']}")
            print(f"âš ï¸ ì´ {result['ingredient_count']}ê°œì˜ ì•Œë ˆë¥´ê¸° ìœ ë°œ ì„±ë¶„ ë°œê²¬!")
            
            print("\nğŸ“ ì „ì²´ OCR í…ìŠ¤íŠ¸:")
            print("-" * 50)
            print(result['full_text'])
            print("-" * 50)
            
            print("\nğŸš¨ ë°œê²¬ëœ ì•Œë ˆë¥´ê¸° ìœ ë°œ ì„±ë¶„:")
            print("-" * 30)
            for i, ingredient in enumerate(result['ingredients'], 1):
                print(f"{i:2d}. {ingredient}")
            
            if result['ingredient_count'] == 0:
                print("âœ… ì•Œë ˆë¥´ê¸° ìœ ë°œ ì„±ë¶„ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
                print("\nğŸ’¡ ì„±ëŠ¥ ê°œì„  íŒ:")
                print("  - EasyOCRì´ ë” ì •í™•í•©ë‹ˆë‹¤")
                print("  - ì´ë¯¸ì§€ í’ˆì§ˆì´ ì¢‹ì„ìˆ˜ë¡ ê²°ê³¼ê°€ í–¥ìƒë©ë‹ˆë‹¤")
                print("  - ë‹¤ë¥¸ ì´ë¯¸ì§€ë¡œë„ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”!")
                
    except Exception as e:
        print("âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜:", e)
